{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Monet GAN project","metadata":{}},{"cell_type":"markdown","source":"The goal of this project is to make 256×256 images that look like Monet's paintings. I used the Kaggle dataset and the monet_jpg folder which contained 300 256×256 rgb images. I built a dcgan, trained a few epochs and generated 7,000 images","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T05:32:20.040106Z","iopub.execute_input":"2025-10-06T05:32:20.040377Z","iopub.status.idle":"2025-10-06T05:32:35.009402Z","shell.execute_reply.started":"2025-10-06T05:32:20.040351Z","shell.execute_reply":"2025-10-06T05:32:35.008473Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data check\nI first checked that the dataset was added and the folders existed. The competition data has four folders: monet_jpg, photo_jpg, monet_tfrec and photo_tfrec. For this notebook I only used monet_jpg. I checked that monet_jpg had 300 images and each one is 256×256 rgb","metadata":{}},{"cell_type":"code","source":"# detect data, set paths and confirm\nimport os\n\nbase = None\nfor d in os.listdir(\"/kaggle/input\"):\n    b = f\"/kaggle/input/{d}\"\n    if os.path.isdir(os.path.join(b, \"monet_jpg\")) and os.path.isdir(os.path.join(b, \"photo_jpg\")):\n        base = b\n        break\n\nassert base is not None, \"Data isn’t there.\"\n\nkaggledat = base\nmonetjpg = os.path.join(kaggledat, \"monet_jpg\")\n\nprint(\"base:\", kaggledat)\nprint(\"monet_jpg is there:\", os.path.isdir(os.path.join(kaggledat,\"monet_jpg\")))\nprint(\"photo_jpg is there:\", os.path.isdir(os.path.join(kaggledat,\"photo_jpg\")))\nprint(\"monet_tfrec is there:\", os.path.isdir(os.path.join(kaggledat,\"monet_tfrec\")))\nprint(\"photo_tfrec is there:\", os.path.isdir(os.path.join(kaggledat,\"photo_tfrec\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T05:32:35.010974Z","iopub.execute_input":"2025-10-06T05:32:35.011463Z","iopub.status.idle":"2025-10-06T05:32:35.028193Z","shell.execute_reply.started":"2025-10-06T05:32:35.011438Z","shell.execute_reply":"2025-10-06T05:32:35.027621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Prep\n\nI loaded the jpegs with tensor flow and scaled pixels to -1, 1. I shuffled the files, did a random left right flip, made batches and then prefetched for speed","metadata":{}},{"cell_type":"code","source":"# define constants and build dataset from monet jpegs\nimgsize = 256\nbatch = 8\nvectlength = 64\nnumepochs = 5\n\nimport os, glob, tensorflow as tf\n\n# read jpeg and and set pixels to -1, 1\ndef readjpg(fname):\n    img = tf.io.read_file(fname)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    return img * 2.0 - 1.0\n\n# build dataset: list the files, lr flip, batch and prefetch\ndef builddataset(folder, batchsize=8):\n    filelist = sorted(glob.glob(os.path.join(folder, \"*.jpg\")))\n    dataobj = tf.data.Dataset.from_tensor_slices(filelist)\n    dataobj = dataobj.shuffle(len(filelist), reshuffle_each_iteration=True)\n    dataobj = dataobj.map(readjpg, num_parallel_calls=tf.data.AUTOTUNE)\n    dataobj = dataobj.map(lambda img: tf.image.random_flip_left_right(img), num_parallel_calls=tf.data.AUTOTUNE)\n    dataobj = dataobj.batch(batchsize, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n    return dataobj, len(filelist)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T05:32:35.028846Z","iopub.execute_input":"2025-10-06T05:32:35.029065Z","iopub.status.idle":"2025-10-06T05:32:54.127872Z","shell.execute_reply.started":"2025-10-06T05:32:35.029047Z","shell.execute_reply":"2025-10-06T05:32:54.127315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model and training\nI used a small dcgan. The generator takes a short random input of 64 numbers and made a 256×256 rgb image with conv-transpose layers and a tanh output. The discriminator is a small cnn that takes an image and outputs one number for real v fake. I trained with binary cross entropy from logits and Adam at 1e-4 for both. Each step I updated the discriminator on real and fake and then updated the generator to trick it. I ran 5 epochs for a baseline","metadata":{}},{"cell_type":"code","source":"# get dataset and print img count\ntraindata, monetcounter = builddataset(monetjpg, batch)\nprint(\"Monet images:\", monetcounter)\n\n# create generator and discriminator, then set up and run gan training loop\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# generator: maps vectors to 256x256 rgb images\ndef makegenerator():\n    g = tf.keras.Sequential(name=\"G\")\n    g.add(layers.Input(shape=(vectlength,)))\n    g.add(layers.Dense(16*16*128, use_bias=False))\n    g.add(layers.BatchNormalization()); g.add(layers.LeakyReLU())\n    g.add(layers.Reshape((16,16,128)))\n    g.add(layers.Conv2DTranspose(128, 4, 2, \"same\", use_bias=False)); g.add(layers.BatchNormalization()); g.add(layers.LeakyReLU())\n    g.add(layers.Conv2DTranspose(64, 4, 2, \"same\", use_bias=False)); g.add(layers.BatchNormalization()); g.add(layers.LeakyReLU())\n    g.add(layers.Conv2DTranspose(32, 4, 2, \"same\", use_bias=False)); g.add(layers.BatchNormalization()); g.add(layers.LeakyReLU())\n    g.add(layers.Conv2DTranspose(16, 4, 2, \"same\", use_bias=False)); g.add(layers.BatchNormalization()); g.add(layers.LeakyReLU())\n    g.add(layers.Conv2D(3, 3, padding=\"same\", activation=\"tanh\"))\n    return g\n\n# discriminator. cnn that outputs one logit for real v fake \ndef makediscrim():\n    d = tf.keras.Sequential(name=\"D\")\n    d.add(layers.Input(shape=(imgsize, imgsize, 3)))\n    d.add(layers.Conv2D(32, 4, 2, \"same\")); d.add(layers.LeakyReLU()); d.add(layers.Dropout(0.3))\n    d.add(layers.Conv2D(64, 4, 2, \"same\")); d.add(layers.LeakyReLU()); d.add(layers.Dropout(0.3))\n    d.add(layers.Conv2D(128, 4, 2, \"same\")); d.add(layers.LeakyReLU()); d.add(layers.Dropout(0.3))\n    d.add(layers.Conv2D(256, 4, 2, \"same\")); d.add(layers.LeakyReLU()); d.add(layers.Dropout(0.3))\n    d.add(layers.Flatten()); d.add(layers.Dense(1))\n    return d\n\n# init models \ngenerator = makegenerator()\ndiscriminator = makediscrim()\n\n# gan set up. bin cross entropy logits and adam optimizers \nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\noptgen = tf.keras.optimizers.Adam(1e-4)\noptdiscrim = tf.keras.optimizers.Adam(1e-4)\n\n# one train step. train discriminator real and fake then train generator to trick it \n@tf.function\ndef trainone(real):\n    bs = tf.shape(real)[0]\n    z = tf.random.normal([bs, vectlength])\n    with tf.GradientTape() as dt:\n        fake = generator(z, training=True)\n        rlog = discriminator(real, training=True)\n        flog = discriminator(fake, training=True)\n        dloss = loss(tf.ones_like(rlog), rlog) + loss(tf.zeros_like(flog), flog)\n    dgr = dt.gradient(dloss, discriminator.trainable_variables)\n    optdiscrim.apply_gradients(zip(dgr, discriminator.trainable_variables))\n\n    z = tf.random.normal([bs, vectlength])\n    with tf.GradientTape() as gt:\n        fake = generator(z, training=True)\n        flog = discriminator(fake, training=True)\n        gloss = loss(tf.ones_like(flog), flog)\n    ggr = gt.gradient(gloss, generator.trainable_variables)\n    optgen.apply_gradients(zip(ggr, generator.trainable_variables))\n\n    return dloss, gloss\n\n# training loop. iterate over data for few epochs and print average loss\ndef runthepochs(data, epochs):\n    for e in range(epochs):\n        dsum = 0.0; gsum = 0.0; steps = 0\n        for real in data:\n            dl, gl = trainone(real)\n            dsum += float(dl); gsum += float(gl); steps += 1\n        print(f\"Epoch {e+1}/{epochs} d_loss={dsum/max(steps,1):.4f} g_loss={gsum/max(steps,1):.4f}\")\n\nrunthepochs(traindata, numepochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T05:32:54.128630Z","iopub.execute_input":"2025-10-06T05:32:54.129121Z","iopub.status.idle":"2025-10-06T05:33:22.259455Z","shell.execute_reply.started":"2025-10-06T05:32:54.129098Z","shell.execute_reply":"2025-10-06T05:33:22.258664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Image generation\nAfter training I sampled 7,000 images from the generator and saved them as 256×256 rgb jpegs","metadata":{}},{"cell_type":"code","source":"# generate images and sub\nfrom pathlib import Path\nimport zipfile\nimport tensorflow as tf\n\nimgwrite = Path(\"/kaggle/working/images\")\nimgwrite.mkdir(parents=True, exist_ok=True)\n\nimgcounter = 7000\nsavestep = 100\n\n# convert -1,1 floats to uint8\ndef tou8(arr):\n    arr = tf.clip_by_value((arr + 1.0) * 127.5, 0, 255)\n    return tf.cast(arr, tf.uint8)\n\nsaved = 0\nidx = 0\nwhile saved < imgcounter:\n    take = min(savestep, imgcounter - saved)\n    z = tf.random.normal([take, vectlength])\n    imgs = generator(z, training=False)\n    imgs = tou8(imgs).numpy()\n    for i in range(take):\n        tf.keras.utils.save_img(str(imgwrite / f\"{idx:05d}.jpg\"), imgs[i])\n        idx += 1\n    saved += take\n    print(\"saved:\", saved)\n\nzipath = Path(\"/kaggle/working/images.zip\")\nwith zipfile.ZipFile(zipath, \"w\", compression=zipfile.ZIP_STORED) as zf:\n    for p in sorted(imgwrite.glob(\"*.jpg\")):\n        zf.write(p, arcname=p.name)\n\nprint(\"wrote:\", zipath, \"bytes:\", zipath.stat().st_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T05:33:22.261771Z","iopub.execute_input":"2025-10-06T05:33:22.262016Z","iopub.status.idle":"2025-10-06T05:33:46.209097Z","shell.execute_reply.started":"2025-10-06T05:33:22.261997Z","shell.execute_reply":"2025-10-06T05:33:46.208329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results\nI trained for 5 epochs on the 300 Monet jpegs. The logs showed the discriminator loss going from 0.8751 to 0.1019 and the generator loss going from 1.5782 to 9.8325. When I looked at a grid of samples they mostly looked like purple/green blobs or textures, not clear scenes. My Kaggle score was 397.92205 MiFID which improved over my first submission 473.80291","metadata":{}},{"cell_type":"markdown","source":"### Conclusion\n\nThe images captured Monet like color and texture but not the structure or form of the Monet paintings which makes sense to me given the small model and short amount of time for training. If I had more time I’d try training longer, slightly changing the optimizer settings and making the networks a bit bigger to see if shapes start to appear","metadata":{}}]}